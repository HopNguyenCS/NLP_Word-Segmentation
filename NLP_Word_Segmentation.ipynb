{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Tải dữ liệu và từ điển"
      ],
      "metadata": {
        "id": "3r6qeYdy-5LA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUULHf9oz25c",
        "outputId": "619b42d4-441f-49d7-9f03-a8384396c33f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Import các thư viện cần thiết\n",
        "from collections import defaultdict\n",
        "import unicodedata as ud\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import ast\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/undertheseanlp/underthesea"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu3se75U4Gpm",
        "outputId": "5d23f568-f8d5-4437-d6ae-5676423e3747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'underthesea'...\n",
            "remote: Enumerating objects: 7185, done.\u001b[K\n",
            "remote: Counting objects: 100% (1404/1404), done.\u001b[K\n",
            "remote: Compressing objects: 100% (863/863), done.\u001b[K\n",
            "remote: Total 7185 (delta 574), reused 1243 (delta 504), pack-reused 5781\u001b[K\n",
            "Receiving objects: 100% (7185/7185), 167.46 MiB | 23.37 MiB/s, done.\n",
            "Resolving deltas: 100% (3669/3669), done.\n",
            "Updating files: 100% (1606/1606), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Từ điển\n",
        "words = open('/content/underthesea/underthesea/corpus/data/Viet74K.txt', encoding='utf-8').readlines()"
      ],
      "metadata": {
        "id": "DrxfX15W4Gr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_words = []\n",
        "for word in words:\n",
        "    new_words.append(word.replace(\"-\",\" \"))"
      ],
      "metadata": {
        "id": "WXvNElLE4Guc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tách thành bi_grams và tri_grams\n",
        "bi_grams = []\n",
        "tri_grams = []\n",
        "for word in new_words:\n",
        "    count_space =0\n",
        "    for chart in word:\n",
        "        if chart ==\" \":\n",
        "            count_space += 1\n",
        "    if count_space == 1:\n",
        "        bi_grams.append(word.replace(\"\\n\",\"\"))\n",
        "        new_words.remove(word)\n",
        "    if count_space == 2:\n",
        "        tri_grams.append(word.replace(\"\\n\",\"\"))\n",
        "        new_words.remove(word)"
      ],
      "metadata": {
        "id": "MNZXrkvX4GwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(bi_grams)"
      ],
      "metadata": {
        "id": "850xJZB6YsfA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96b648d1-2fc4-4115-8484-9eae861d912b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27938"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tri_grams)"
      ],
      "metadata": {
        "id": "iuurf-0xYsiP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76259196-1ef1-47a8-b879-c69d412a96c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3402"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ngữ liệu thu thập\n",
        "sentences = open('/content/cau.txt', encoding='utf-8').readlines()\n",
        "tokenize_sentences = [sentence.split(' ') for sentence in sentences]"
      ],
      "metadata": {
        "id": "XsVeSvODAeJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Số lượng câu đã thu thập:', len(sentences))\n",
        "sentences[0:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22THc3K2_xP9",
        "outputId": "b4a6c53a-d54a-4059-fb70-6d5bd95b7b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Số lượng câu đã thu thập: 95\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Kìa mây mây ngang đầu.\\n', 'Kìa núi núi lô nhô.\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_sentence = max(tokenize_sentences, key=len)\n",
        "print('Câu có số từ nhiều nhất:', len(max_sentence))\n",
        "' '.join(max_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "-jRRRwgzBFOv",
        "outputId": "3afdb7a9-d36a-4250-dd5e-5ad523a512fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Câu có số từ nhiều nhất: 31\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hai nhà lãnh đạo cũng trao đổi một số biện pháp nhằm tạo điều kiện thuận lợi cho doanh nghiệp mỗi nước tiếp cận thị trường và đầu tư kinh doanh.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_sentence = min(tokenize_sentences, key=len)\n",
        "print('Câu có số từ ít nhất:', len(min_sentence))\n",
        "' '.join(min_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "mNiqnRQm_31r",
        "outputId": "b7bc1118-71bd-452f-a3ff-ac9585590edb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Câu có số từ ít nhất: 5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Kìa mây mây ngang đầu.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Thuật toán Longest-Matching\n"
      ],
      "metadata": {
        "id": "OQQ-ghL5_RYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def syllablize(sentence):\n",
        "    word_pattern = r'\\w+'                    # tương ứng với bất kỳ ký tự từ (chữ cái, chữ số, dấu gạch dưới)\n",
        "    non_word_pattern = r'[^\\w\\s]'            # tìm các ký từ không phải từ\n",
        "    digits_pattern = r'\\d+([\\.,_]\\d+)+'      # tìm các số trong câu\n",
        "\n",
        "    patterns = [word_pattern, non_word_pattern, digits_pattern] # danh sách các biểu thức chính quy\n",
        "    combined_pattern = f\"({'|'.join(patterns)})\"\n",
        "\n",
        "    sentence = ud.normalize('NFC', sentence)\n",
        "    tokens = re.findall(combined_pattern, sentence, re.UNICODE)  # tìm tất cả chuỗi phù hợp với patterns\n",
        "    return [token[0] for token in tokens]"
      ],
      "metadata": {
        "id": "N3d430yC4G2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_n_grams(path):\n",
        "    with open(path, encoding='utf8') as f:\n",
        "        words = f.read()\n",
        "        words = ast.literal_eval(words)\n",
        "    return words"
      ],
      "metadata": {
        "id": "yra2PIpT_WHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def longest_matching(sentence, bi_grams, tri_grams):\n",
        "    syllables = syllablize(sentence)\n",
        "    syl_len = len(syllables)\n",
        "    curr_id = 0\n",
        "    word_list = []\n",
        "    # duyệt qua các âm tiết trong câu\n",
        "    while curr_id < syl_len:\n",
        "        curr_word = syllables[curr_id]\n",
        "        if curr_id >= syl_len - 1:   # kiểm tra âm tiết hiện tại có phải là âm tiết cuối cùng trong câu hay không\n",
        "            word_list.append(curr_word)  # nếu là âm tiết cuối cùng thì thêm vào word\n",
        "            break\n",
        "\n",
        "        next_word = syllables[curr_id + 1]  # âm tiết kế tiếp trong câu\n",
        "        pair_word = ' '.join([curr_word.lower(), next_word.lower()])\n",
        "\n",
        "        if curr_id >= (syl_len - 2):\n",
        "            if pair_word in bi_grams:  # nếu pair_word có trong bi_grams\n",
        "                word_list.append('_'.join([curr_word, next_word]))   # thì thêm vào word_list bằng cách nối curr_word và next_word bằng dấu gạch dưới\n",
        "                curr_id += 2\n",
        "            else:\n",
        "                word_list.append(curr_word) # âm tiết hiện tại được thêm vào word_list\n",
        "                curr_id += 1\n",
        "        else:\n",
        "            next_next_word = syllables[curr_id + 2]\n",
        "            triple_word = ' '.join([pair_word, next_next_word.lower()])\n",
        "\n",
        "            if triple_word in tri_grams:   # nếu triple_word có trong tri_grams\n",
        "                word_list.append('_'.join([curr_word, next_word, next_next_word]))   # thì thêm vào word_list bằng cách nối curr_word, next_word và next_next_word bằng dấu gạch dưới\n",
        "                curr_id += 3\n",
        "            elif pair_word in bi_grams:\n",
        "                word_list.append('_'.join([curr_word, next_word]))\n",
        "                curr_id += 2\n",
        "            else:\n",
        "                word_list.append(curr_word)\n",
        "                curr_id += 1\n",
        "    return word_list"
      ],
      "metadata": {
        "id": "6ATxg7V7_WKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/cau_longestmatching.txt', 'w', encoding='utf-8') as f:\n",
        "    longest_matching_sentences = []\n",
        "    for sentence in sentences:\n",
        "        word_list = longest_matching(sentence, bi_grams, tri_grams)\n",
        "        longest_matching_sentences.append(' '.join(word_list))\n",
        "        for word in word_list: f.write(word + '\\n')\n",
        "        if sentence != sentences[-1]: f.write('\\n')\n",
        "    f.write('\\n')\n",
        "longest_matching_compound_words = 0\n",
        "for sentence in longest_matching_sentences:\n",
        "    for word in sentence.split():\n",
        "        if '_' in word: longest_matching_compound_words += 1\n",
        "print('Số lượng từ ghép khi tách từ bằng thuật toán Longest Matching:', longest_matching_compound_words)\n",
        "longest_matching_sentences[10:15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU8rKQAO_WNO",
        "outputId": "ca7b2f74-f440-449c-9c43-ab77f4733ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Số lượng từ ghép khi tách từ bằng thuật toán Longest Matching: 186\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Lên trên này thấy các em , anh mới thấy mình quá may_mắn .',\n",
              " 'Mặt_trời vàng như trứng chiên .',\n",
              " 'Khi mà em đứng thẳng , núi và đồi phải đứng nghiêng .',\n",
              " 'Con_đường quanh_co quá , em chao lượn như cánh chim .',\n",
              " 'Mai_sau lớn em bơi giữa đời , em bơi khỏe như Ánh Viên .']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Thư viện Underthesea"
      ],
      "metadata": {
        "id": "IbFCzq1i5Fss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install underthesea"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mrHkS5mApdP",
        "outputId": "72d232af-eacb-48fb-946c-da1c28c1cdbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting underthesea\n",
            "  Downloading underthesea-6.3.0-py3-none-any.whl (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from underthesea) (8.1.3)\n",
            "Collecting python-crfsuite>=0.9.6 (from underthesea)\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from underthesea) (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from underthesea) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from underthesea) (2.27.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.2.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.2.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from underthesea) (6.0)\n",
            "Collecting underthesea-core==1.0.4 (from underthesea)\n",
            "  Downloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl (657 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->underthesea) (2022.10.31)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (3.1.0)\n",
            "Installing collected packages: underthesea-core, python-crfsuite, underthesea\n",
            "Successfully installed python-crfsuite-0.9.9 underthesea-6.3.0 underthesea-core-1.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from underthesea import word_tokenize"
      ],
      "metadata": {
        "id": "du71e1FKApkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/cau_underthesea.txt', 'w', encoding='utf-8') as f:\n",
        "  underthesea_sentences = []\n",
        "  for sentence in sentences:\n",
        "      word_list = word_tokenize(str(sentence).strip(),format=\"text\") #Tách câu thành danh sách các từ bằng cách sử dụng hàm 'word_tokenize'\n",
        "      word_list = word_list.split(\" \")\n",
        "      underthesea_sentences.append(' '.join(word_list))\n",
        "      for word in word_list:\n",
        "        f.write(word + '\\n')\n",
        "      if sentence != sentences[-1]: f.write('\\n')\n",
        "  f.write('\\n')\n",
        "underthesea_compound_words = 0\n",
        "for sentence in underthesea_sentences:\n",
        "    for word in sentence:\n",
        "        if '_' in word:\n",
        "          underthesea_compound_words += 1\n",
        "print('Số lượng từ ghép khi tách từ bằng thư viện underthesea:', underthesea_compound_words)\n",
        "underthesea_sentences[10:15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x52GzI8QApog",
        "outputId": "09559faa-ba0d-432e-98ec-c33907ba6965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Số lượng từ ghép khi tách từ bằng thư viện underthesea: 280\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Lên trên này thấy các em , anh mới thấy mình quá may_mắn .',\n",
              " 'Mặt_trời vàng như trứng chiên .',\n",
              " 'Khi mà em đứng_thẳng , núi và đồi phải đứng nghiêng .',\n",
              " 'Con đường quanh_co quá , em chao lượn như cánh chim .',\n",
              " 'Mai sau lớn em bơi giữa đời , em bơi khỏe như Ánh_Viên .']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tách từ thủ công"
      ],
      "metadata": {
        "id": "DSj4ePLi5T62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/final_tokens.txt', 'r', encoding='utf-8') as f:\n",
        "    final_tokenize_sentences = []\n",
        "    sentence = ''\n",
        "    for word in f:\n",
        "        if word == '\\n':\n",
        "            final_tokenize_sentences.append(sentence.strip())\n",
        "            sentence = ''\n",
        "        else: sentence += word.replace('\\n', ' ')\n",
        "final_tokenize_compound_words = 0\n",
        "for sentence in final_tokenize_sentences:\n",
        "    for word in sentence.split():\n",
        "        if '_' in word: final_tokenize_compound_words += 1\n",
        "print('Số lượng từ ghép khi tách từ thủ công:', final_tokenize_compound_words)\n",
        "final_tokenize_sentences[10:15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-88I_tBY6Ov",
        "outputId": "1327b573-85c8-4a7e-9d0f-bffdcd82abce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Số lượng từ ghép khi tách từ thủ công: 267\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lên trên này thấy các em , anh mới thấy mình quá may_mắn .',\n",
              " 'mặt_trời vàng như trứng chiên .',\n",
              " 'khi mà em đứng thẳng , núi và đồi phải đứng nghiêng .',\n",
              " 'con đường quanh_co quá , em chao lượn như cánh chim .',\n",
              " 'mai_sau lớn em bơi giữa đời , em bơi khỏe như Ánh_Viên .']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Đánh giá\n"
      ],
      "metadata": {
        "id": "Y9zWeBH0ZOR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_correct_words(pred, source, n_grams=3):\n",
        "    pred_words = pred.split()\n",
        "    source_words = source.split()\n",
        "    total_true = 0\n",
        "    true_positive = 0\n",
        "    total_errors = 0\n",
        "    false_positive = 0\n",
        "    index = 0\n",
        "    while index < len(pred_words):\n",
        "        if pred_words[index] not in source_words[index:(index + n_grams)]:\n",
        "            if '_' in pred_words[index]:\n",
        "                false_positive += 1\n",
        "            del pred_words[index]\n",
        "            total_errors += 1\n",
        "        else:\n",
        "            index += 1\n",
        "    index = 0\n",
        "    while index < len(source_words):\n",
        "        if source_words[index] not in pred_words[index:(index + n_grams)]:\n",
        "            del source_words[index]\n",
        "        else:\n",
        "            index += 1\n",
        "    words = pred_words if len(pred_words) < len(source_words) else source_words\n",
        "    for index in range(len(words)):\n",
        "        if pred_words[index] == source_words[index]:\n",
        "            if '_' in pred_words[index]:\n",
        "                true_positive += 1\n",
        "            total_true += 1\n",
        "    return total_true, total_errors, true_positive, false_positive"
      ],
      "metadata": {
        "id": "qqGFpGtESThY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_evaluation(pred, source, n_grams=3):\n",
        "    total_true = 0\n",
        "    total_errors = 0\n",
        "    total_words = 0\n",
        "    pred_tp = 0\n",
        "    pred_fp = 0\n",
        "    for pred_sentence, source_sentence in zip(pred, source):\n",
        "        total_words += len(source_sentence.split())\n",
        "        if pred_sentence != source_sentence:\n",
        "            result = count_correct_words(pred_sentence, source_sentence, n_grams)\n",
        "            total_true += result[0]\n",
        "            total_errors += result[1]\n",
        "            pred_tp += result[2]\n",
        "            pred_fp += result[3]\n",
        "        else:\n",
        "            for word in source_sentence.split():\n",
        "                if '_' in word:\n",
        "                    pred_tp += 1\n",
        "                total_true += 1\n",
        "    return {\n",
        "        'Accuracy': total_true / total_words,\n",
        "        'Precision': pred_tp / (pred_tp + pred_fp),\n",
        "        'Recall': pred_tp / final_tokenize_compound_words,\n",
        "    }"
      ],
      "metadata": {
        "id": "vNknz5VCYpRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "longest_matching_evaluation = tokenize_evaluation(longest_matching_sentences, final_tokenize_sentences)\n",
        "underthesea_evaluation = tokenize_evaluation(underthesea_sentences, final_tokenize_sentences)\n",
        "pd.DataFrame([longest_matching_evaluation, underthesea_evaluation],index = ['Longest Matching','Underthesea']).astype(object).T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "I5_kMEMgYpUd",
        "outputId": "12533a3b-c2d8-40a9-d565-5f5e3fcac52e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Longest Matching Underthesea\n",
              "Accuracy          0.587102    0.743719\n",
              "Precision         0.505435    0.653285\n",
              "Recall            0.348315    0.670412"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7ecfaf11-3670-4be4-b436-cc71c55a07d6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Longest Matching</th>\n",
              "      <th>Underthesea</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Accuracy</th>\n",
              "      <td>0.587102</td>\n",
              "      <td>0.743719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Precision</th>\n",
              "      <td>0.505435</td>\n",
              "      <td>0.653285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Recall</th>\n",
              "      <td>0.348315</td>\n",
              "      <td>0.670412</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7ecfaf11-3670-4be4-b436-cc71c55a07d6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7ecfaf11-3670-4be4-b436-cc71c55a07d6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7ecfaf11-3670-4be4-b436-cc71c55a07d6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}